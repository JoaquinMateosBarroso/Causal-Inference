{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370075c8",
   "metadata": {},
   "source": [
    "## Transforming data in csv and parents format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5623a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Name: pm25\n",
      "Shape of Graph H: (36, 36)\n",
      "Shape of Time-series Data: (480, 20, 36) (Sample_num, Time_step, Node_num)\n",
      "Data Name: medical\n",
      "Shape of Graph H: (20, 20)\n",
      "Shape of Time-series Data: (480, 20, 20) (Sample_num, Time_step, Node_num)\n",
      "Data Name: traffic\n",
      "Shape of Graph H: (20, 20)\n",
      "Shape of Time-series Data: (480, 20, 20) (Sample_num, Time_step, Node_num)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "import pandas as pd\n",
    "sys.path.append('../')\n",
    "\n",
    "data_names = ['pm25', 'medical', 'traffic']\n",
    "\n",
    "def matrix_graph_to_parents(matrix_graph: np.ndarray) -> dict[int, list[int]]:\n",
    "    \"\"\"\n",
    "    Convert a matrix graph to a dictionary representation of parents.\n",
    "\n",
    "    Args:\n",
    "        matrix_graph (np.ndarray): The adjacency matrix representing the graph.\n",
    "\n",
    "    Returns:\n",
    "        dict[int, list[int]]: A dictionary where keys are node indices and values are lists of parent node indices.\n",
    "    \"\"\"\n",
    "    parents = {}\n",
    "    for i in range(matrix_graph.shape[0]):\n",
    "        parents[i] = np.where(matrix_graph[i] == 1)[0].tolist()\n",
    "    \n",
    "    parents = {k: [(v, -1) for v in vs] for k, vs in parents.items()}\n",
    "    return parents\n",
    "\n",
    "SAMPLE_NUM = 480 # All the datasets have 480 samples\n",
    "for data_name in data_names:\n",
    "    data = np.load('./' + data_name + '/gen_data.npy')\n",
    "    data = data[:, 20:, :data.shape[2] // 2]  # Forget the residuals and for some reason first 20 values are random\n",
    "    matrix_graph = np.load('./' + data_name + '/graph.npy')\n",
    "\n",
    "    print(f\"Data Name: {data_name}\")\n",
    "    print(f'Shape of Graph H: {matrix_graph.shape}')\n",
    "    print(f'Shape of Time-series Data: {data.shape} (Sample_num, Time_step, Node_num)')\n",
    "\n",
    "    os.makedirs(f'./data_{data_name}', exist_ok=True)\n",
    "    for i in range(data.shape[0]): # Iterate over samples\n",
    "        with open(f'./data_{data_name}/{i}_node_parents.txt', 'w') as f:\n",
    "            f.write(str(matrix_graph_to_parents(matrix_graph))) # Same matrix graph for all samples\n",
    "        \n",
    "        current_data = data[i, :, :]  # Select the i-th sample\n",
    "        \n",
    "        pd.DataFrame(current_data).to_csv(f'./data_{data_name}/{i}_data.csv', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d0e3ac",
   "metadata": {},
   "source": [
    "## Find the groups we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "788f8552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm25 dataset obtained the groups: [{4}, {1, 29, 5, 33}, {2}, {11, 3, 7}, {34, 6, 10, 14, 18}, {8, 16, 12, 0}, {9}, {13, 17, 20, 21, 24, 25}, {19, 23, 15}, {26, 22}, {35, 27, 28, 31}, {30}, {32}]\n",
      "medical dataset obtained the groups: [{17, 2}, {0, 1, 5, 7, 16, 18, 19}, {9, 3, 14}, {4, 6, 8, 10, 11, 12, 13, 15}]\n",
      "traffic dataset obtained the groups: [{8}, {0, 1, 2, 3, 4, 6, 7, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {5}]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "from group_causation.groups_extraction import GeneticCausalGroupsExtractor\n",
    "\n",
    "\n",
    "datasets_groups = {k: None for k in data_names}\n",
    "def extract_and_save_groups(data_name):\n",
    "    data = pd.read_csv(f'./data_{data_name}/0_data.csv', header=None).values\n",
    "    if data.shape[1] > 30: # Since there are many variables, consider the harmonic variance\n",
    "        group_extractor = GeneticCausalGroupsExtractor(data, \n",
    "                                                    scores=['harmonic_variance_explained', 'explainability_score'], \n",
    "                                                    scores_weights=[0.1, 1.0])\n",
    "    else:\n",
    "        group_extractor = GeneticCausalGroupsExtractor(data, \n",
    "                                                    scores=['explainability_score'], \n",
    "                                                    scores_weights=[1.0])\n",
    "        \n",
    "    groups = group_extractor.extract_groups()\n",
    "    datasets_groups[data_name] = groups\n",
    "\n",
    "    print(data_name, 'dataset obtained the groups:', groups)\n",
    "    \n",
    "    with open(f'./data_{data_name}/0_groups.txt', 'w') as f:\n",
    "        f.write(str(groups))\n",
    "    \n",
    "    for i in range(1, SAMPLE_NUM):\n",
    "        shutil.copyfile(f'./data_{data_name}/0_groups.txt', f'./data_{data_name}/{i}_groups.txt')\n",
    "\n",
    "for data_name in data_names:\n",
    "    extract_and_save_groups(data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e12a67",
   "metadata": {},
   "source": [
    "## Convert node-level parents to group-level parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "982b4af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_with_element(groups, x):\n",
    "    for i, group in enumerate(groups):\n",
    "        if x in group: return i\n",
    "    return None\n",
    "\n",
    "for data_name, groups in datasets_groups.items():\n",
    "    with open(f'./data_{data_name}/0_node_parents.txt', 'r') as f:\n",
    "        node_parents = eval(f.read())\n",
    "    \n",
    "    group_parents = {}\n",
    "    for son_group_idx, son_group in enumerate(groups):\n",
    "        group_parents[son_group_idx] = []\n",
    "        for son_node in son_group:\n",
    "            for parent_node in node_parents[son_node]:\n",
    "                parent_group_idx = find_index_with_element(groups, parent_node)\n",
    "                if (parent_group_idx, -1) not in group_parents[son_group_idx]:\n",
    "                    group_parents[son_group_idx].append((parent_group_idx, -1))\n",
    "                \n",
    "    with open(f'./data_{data_name}/0_parents.txt', 'w') as f:\n",
    "        f.write(str(group_parents))\n",
    "    for i in range(1, SAMPLE_NUM):\n",
    "        shutil.copyfile(f'./data_{data_name}/0_parents.txt', f'./data_{data_name}/{i}_parents.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088e762",
   "metadata": {},
   "source": [
    "## Perform the benchmark for each of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d3be1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from group_causation.benchmark import BenchmarkGroupCausalDiscovery\n",
    "\n",
    "from group_causation.utils import static_parameters\n",
    "from group_causation.group_causal_discovery import DimensionReductionGroupCausalDiscovery\n",
    "from group_causation.group_causal_discovery import MicroLevelGroupCausalDiscovery\n",
    "from group_causation.group_causal_discovery import HybridGroupCausalDiscovery\n",
    "\n",
    "algorithms = {\n",
    "    'group-embedding': HybridGroupCausalDiscovery,\n",
    "    'subgroups': HybridGroupCausalDiscovery,\n",
    "    'pca+pcmci': DimensionReductionGroupCausalDiscovery,\n",
    "    'pca+dynotears': DimensionReductionGroupCausalDiscovery,\n",
    "    'micro-level': MicroLevelGroupCausalDiscovery,\n",
    "}\n",
    "algorithms_parameters = {\n",
    "    'pca+pcmci': {'dimensionality_reduction': 'pca', 'node_causal_discovery_alg': 'pcmci',\n",
    "                            'node_causal_discovery_params': {'min_lag': 1, 'max_lag': 3, 'pc_alpha': 0.05}},\n",
    "    \n",
    "    'pca+dynotears': {'dimensionality_reduction': 'pca', 'node_causal_discovery_alg': 'dynotears',\n",
    "                            'node_causal_discovery_params': {'max_lag': 3, 'lambda_w': 0.05, 'lambda_a': 0.05}},\n",
    "    \n",
    "    'micro-level': {'node_causal_discovery_alg': 'pcmci',\n",
    "                            'node_causal_discovery_params': {'min_lag': 1, 'max_lag': 3, 'pc_alpha': 0.05}},\n",
    "    \n",
    "    'group-embedding': {'dimensionality_reduction': 'pca', \n",
    "               'dimensionality_reduction_params': {'explained_variance_threshold': 0.3,\n",
    "                                                   'groups_division_method': 'group_embedding'},\n",
    "                'node_causal_discovery_alg': 'pcmci',\n",
    "                'node_causal_discovery_params': {'min_lag': 1, 'max_lag': 3, 'pc_alpha': 0.05},\n",
    "                'verbose': 0},\n",
    "    \n",
    "    'subgroups': {'dimensionality_reduction': 'pca', \n",
    "               'dimensionality_reduction_params': {'explained_variance_threshold': 0.3,\n",
    "                                                   'groups_division_method': 'subgroups'},\n",
    "                'node_causal_discovery_alg': 'pcmci',\n",
    "                'node_causal_discovery_params': {'min_lag': 1, 'max_lag': 3, 'pc_alpha': 0.05},\n",
    "                'verbose': 0},\n",
    "}\n",
    "\n",
    "data_generation_options = {}\n",
    "\n",
    "benchmark_options = {\n",
    "    'static_parameters': (static_parameters, {}),\n",
    "}\n",
    "\n",
    "chosen_option = 'static_parameters'\n",
    "\n",
    "\n",
    "def execute_benchmark(data_name):\n",
    "    plt.style.use('default')\n",
    "    plt.rcParams['text.usetex'] = True\n",
    "    plt.rcParams['font.family'] = 'serif'\n",
    "    \n",
    "    benchmark = BenchmarkGroupCausalDiscovery()\n",
    "    results_folder = f'results_{data_name}'\n",
    "    datasets_folder = f'data_{data_name}'\n",
    "    \n",
    "    options_generator, options_kwargs = benchmark_options[chosen_option]\n",
    "    parameters_iterator = options_generator(data_generation_options,\n",
    "                                                algorithms_parameters,\n",
    "                                                **options_kwargs)\n",
    "    results = benchmark.benchmark_causal_discovery(algorithms=algorithms,\n",
    "                                        parameters_iterator=parameters_iterator,\n",
    "                                        datasets_folder=datasets_folder,\n",
    "                                        generate_toy_data=False,\n",
    "                                        results_folder=results_folder,\n",
    "                                        n_executions=480,\n",
    "                                        verbose=1)\n",
    "    \n",
    "    return results, benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d1b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing benchmark of pm25\n",
      "\n",
      "--------------------------------------------------\n",
      "\u001b[34m Datasets have been loaded. \u001b[0m\n",
      "\u001b[32m Executing algorithm group-embedding \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 197/480 [01:30<02:23,  1.97it/s]"
     ]
    }
   ],
   "source": [
    "for data_name in (data_names:=['pm25', 'medical', 'traffic']):\n",
    "    print('Executing benchmark of', data_name)\n",
    "    results, benchmark = execute_benchmark(data_name)\n",
    "    results_folder = f'results_{data_name}'\n",
    "    \n",
    "    # Plot graphs\n",
    "    benchmark.plot_particular_result(results_folder, results_folder + '/summary',\n",
    "                                    scores=[f'{score}_summary' for score in \\\n",
    "                                                    ['shd', 'f1', 'precision', 'recall']],\n",
    "                                    dataset_iteration_to_plot=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
