{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3887310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_name = 'UltraProcessed_Food'\n",
    "data = pd.read_csv(f'{data_name}/DatasetUF.csv', index_col=0)\n",
    "data.to_csv(f'data_{data_name}/0_data.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "def adjacency_to_parents_dict(filename):\n",
    "    parents_dict = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for child_index, line in enumerate(lines):\n",
    "            values = list(map(int, line.strip().split('\\t')))\n",
    "            parents_dict[child_index] = [(child_index,-1)]\n",
    "            for parent_index, value in enumerate(values):\n",
    "                if value == 1:\n",
    "                    parents_dict[child_index].append((parent_index,-1))\n",
    "    return parents_dict\n",
    "\n",
    "parents = adjacency_to_parents_dict(f'{data_name}/UFGroundTruth.txt')\n",
    "with open(f'data_{data_name}/0_node_parents.txt', 'w') as f:\n",
    "    f.write(str(parents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab0b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from group_causation.groups_extraction import GeneticCausalGroupsExtractor\n",
    "\n",
    "\n",
    "def extract_and_save_groups(data_name):\n",
    "    data = pd.read_csv(f'./data_{data_name}/0_data.csv', header=None).values\n",
    "    # group_extractor = GeneticCausalGroupsExtractor(data, \n",
    "    #                                             scores=['harmonic_variance_explained', 'explainability_score'], \n",
    "    #                                             scores_weights=[0.01, 1.0])\n",
    "    group_extractor = GeneticCausalGroupsExtractor(data, \n",
    "                                                scores=['explainability_score'], \n",
    "                                                scores_weights=[1.0])\n",
    "        \n",
    "    groups = group_extractor.extract_groups()\n",
    "\n",
    "    print(data_name, 'dataset obtained the groups:', groups)\n",
    "    \n",
    "    with open(f'./data_{data_name}/0_groups.txt', 'w') as f:\n",
    "        f.write(str(groups))\n",
    "    \n",
    "\n",
    "# extract_and_save_groups(data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e28da",
   "metadata": {},
   "source": [
    "## Convert node-level parents to group-level parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc1b1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_with_element(groups, x):\n",
    "    for i, group in enumerate(groups):\n",
    "        if x in group: return i\n",
    "    return None\n",
    "\n",
    "with open(f'./data_{data_name}/0_groups.txt', 'r') as f:\n",
    "    groups = eval(f.read())\n",
    "with open(f'./data_{data_name}/0_node_parents.txt', 'r') as f:\n",
    "    node_parents = eval(f.read())\n",
    "\n",
    "group_parents = {}\n",
    "for son_group_idx, son_group in enumerate(groups):\n",
    "    group_parents[son_group_idx] = []\n",
    "    for son_node in son_group:\n",
    "        for parent_node, lag in node_parents[son_node]:\n",
    "            parent_group_idx = find_index_with_element(groups, parent_node)\n",
    "            if parent_group_idx is None:\n",
    "                print(f'Error: parent node {parent_node} not found in any group')\n",
    "                continue\n",
    "            if (parent_group_idx, lag) not in group_parents[son_group_idx]:\n",
    "                group_parents[son_group_idx].append((parent_group_idx, -1))\n",
    "            \n",
    "with open(f'./data_{data_name}/0_parents.txt', 'w') as f:\n",
    "    f.write(str(group_parents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a0ce1",
   "metadata": {},
   "source": [
    "## Perform the Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23116155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from group_causation.benchmark import BenchmarkGroupCausalDiscovery\n",
    "\n",
    "from group_causation.utils import static_parameters\n",
    "from group_causation.group_causal_discovery import DimensionReductionGroupCausalDiscovery\n",
    "from group_causation.group_causal_discovery import MicroLevelGroupCausalDiscovery\n",
    "from group_causation.group_causal_discovery import HybridGroupCausalDiscovery\n",
    "\n",
    "algorithms = {\n",
    "    'group-embedding': HybridGroupCausalDiscovery,\n",
    "    'subgroups': HybridGroupCausalDiscovery,\n",
    "    'pca+pcmci': DimensionReductionGroupCausalDiscovery,\n",
    "    'pca+dynotears': DimensionReductionGroupCausalDiscovery,\n",
    "    'micro-level': MicroLevelGroupCausalDiscovery,\n",
    "}\n",
    "algorithms_parameters = {\n",
    "    'pca+pcmci': {'dimensionality_reduction': 'pca', 'node_causal_discovery_alg': 'pcmci',\n",
    "                            'node_causal_discovery_params': {'min_lag': 1, 'max_lag': 3, 'pc_alpha': 0.05}},\n",
    "    \n",
    "    'pca+dynotears': {'dimensionality_reduction': 'pca', 'node_causal_discovery_alg': 'dynotears',\n",
    "                            'node_causal_discovery_params': {'min_lag': 1, 'max_lag': 3, 'lambda_w': 0.05, 'lambda_a': 0.05}},\n",
    "    \n",
    "    'micro-level': {'node_causal_discovery_alg': 'pcmci',\n",
    "                            'node_causal_discovery_params': {'min_lag': 1, 'max_lag': 3, 'pc_alpha': 0.05}},\n",
    "    \n",
    "    'group-embedding': {'dimensionality_reduction': 'pca', \n",
    "               'dimensionality_reduction_params': {'explained_variance_threshold': 0.7,\n",
    "                                                   'groups_division_method': 'group_embedding'},\n",
    "                'node_causal_discovery_alg': 'pcmci',\n",
    "                'node_causal_discovery_params': {'min_lag': 1, 'max_lag': 3, 'pc_alpha': 0.05},\n",
    "                'verbose': 0},\n",
    "    \n",
    "    'subgroups': {'dimensionality_reduction': 'pca', \n",
    "               'dimensionality_reduction_params': {'explained_variance_threshold': 0.7,\n",
    "                                                   'groups_division_method': 'subgroups'},\n",
    "                'node_causal_discovery_alg': 'pcmci',\n",
    "                'node_causal_discovery_params': {'min_lag': 1, 'max_lag': 3, 'pc_alpha': 0.05},\n",
    "                'verbose': 0},\n",
    "}\n",
    "\n",
    "data_generation_options = {}\n",
    "\n",
    "benchmark_options = {\n",
    "    'static_parameters': (static_parameters, {}),\n",
    "}\n",
    "\n",
    "chosen_option = 'static_parameters'\n",
    "\n",
    "\n",
    "def execute_benchmark(data_name):    \n",
    "    benchmark = BenchmarkGroupCausalDiscovery()\n",
    "    results_folder = f'results_{data_name}'\n",
    "    datasets_folder = f'data_{data_name}'\n",
    "    \n",
    "    options_generator, options_kwargs = benchmark_options[chosen_option]\n",
    "    parameters_iterator = options_generator(data_generation_options,\n",
    "                                                algorithms_parameters,\n",
    "                                                **options_kwargs)\n",
    "    results = benchmark.benchmark_causal_discovery(algorithms=algorithms,\n",
    "                                        parameters_iterator=parameters_iterator,\n",
    "                                        datasets_folder=datasets_folder,\n",
    "                                        generate_toy_data=False,\n",
    "                                        results_folder=results_folder,\n",
    "                                        n_executions=5,\n",
    "                                        verbose=1)\n",
    "    \n",
    "    return results, benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c6f570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\u001b[34m Datasets have been loaded. \u001b[0m\n",
      "\u001b[32m Executing algorithm group-embedding \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:25<00:00, 25.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m Executing algorithm subgroups \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:55<00:00, 55.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m Executing algorithm pca+pcmci \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m Executing algorithm pca+dynotears \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m Executing algorithm micro-level \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:32<00:00, 92.67s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'group-embedding': [{'time': 25.81352472305298,\n",
       "    'memory': 339.49696,\n",
       "    'TP': 12,\n",
       "    'FP': 23,\n",
       "    'FN': 2,\n",
       "    'precision': 0.34285714285714286,\n",
       "    'recall': 0.8571428571428571,\n",
       "    'f1': 0.4897959183673469,\n",
       "    'shd': 25,\n",
       "    'TP_summary': 13,\n",
       "    'FP_summary': 2,\n",
       "    'FN_summary': 1,\n",
       "    'precision_summary': 0.8666666666666667,\n",
       "    'recall_summary': 0.9285714285714286,\n",
       "    'f1_summary': 0.896551724137931,\n",
       "    'shd_summary': 3,\n",
       "    'dataset_iteration': 0}],\n",
       "  'subgroups': [{'time': 55.907198429107666,\n",
       "    'memory': 346.73049599999996,\n",
       "    'TP': 12,\n",
       "    'FP': 28,\n",
       "    'FN': 2,\n",
       "    'precision': 0.3,\n",
       "    'recall': 0.8571428571428571,\n",
       "    'f1': 0.4444444444444444,\n",
       "    'shd': 30,\n",
       "    'TP_summary': 13,\n",
       "    'FP_summary': 2,\n",
       "    'FN_summary': 1,\n",
       "    'precision_summary': 0.8666666666666667,\n",
       "    'recall_summary': 0.9285714285714286,\n",
       "    'f1_summary': 0.896551724137931,\n",
       "    'shd_summary': 3,\n",
       "    'dataset_iteration': 0}],\n",
       "  'pca+pcmci': [{'time': 3.3376758098602295,\n",
       "    'memory': 347.05408,\n",
       "    'TP': 8,\n",
       "    'FP': 15,\n",
       "    'FN': 6,\n",
       "    'precision': 0.34782608695652173,\n",
       "    'recall': 0.5714285714285714,\n",
       "    'f1': 0.4324324324324324,\n",
       "    'shd': 21,\n",
       "    'TP_summary': 11,\n",
       "    'FP_summary': 1,\n",
       "    'FN_summary': 3,\n",
       "    'precision_summary': 0.9166666666666666,\n",
       "    'recall_summary': 0.7857142857142857,\n",
       "    'f1_summary': 0.8461538461538461,\n",
       "    'shd_summary': 4,\n",
       "    'dataset_iteration': 0}],\n",
       "  'pca+dynotears': [{'time': 1.0607929229736328,\n",
       "    'memory': 349.454336,\n",
       "    'TP': 0,\n",
       "    'FP': 0,\n",
       "    'FN': 14,\n",
       "    'precision': 0,\n",
       "    'recall': 0.0,\n",
       "    'f1': 0,\n",
       "    'shd': 14,\n",
       "    'TP_summary': 0,\n",
       "    'FP_summary': 0,\n",
       "    'FN_summary': 14,\n",
       "    'precision_summary': 0,\n",
       "    'recall_summary': 0.0,\n",
       "    'f1_summary': 0,\n",
       "    'shd_summary': 14,\n",
       "    'dataset_iteration': 0}],\n",
       "  'micro-level': [{'time': 92.64542531967163,\n",
       "    'memory': 352.59187199999997,\n",
       "    'TP': 13,\n",
       "    'FP': 29,\n",
       "    'FN': 1,\n",
       "    'precision': 0.30952380952380953,\n",
       "    'recall': 0.9285714285714286,\n",
       "    'f1': 0.46428571428571436,\n",
       "    'shd': 30,\n",
       "    'TP_summary': 14,\n",
       "    'FP_summary': 2,\n",
       "    'FN_summary': 0,\n",
       "    'precision_summary': 0.875,\n",
       "    'recall_summary': 1.0,\n",
       "    'f1_summary': 0.9333333333333333,\n",
       "    'shd_summary': 2,\n",
       "    'dataset_iteration': 0}]},\n",
       " <group_causation.benchmark.BenchmarkGroupCausalDiscovery at 0x78a67f7b84c0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_benchmark(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a3d6a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrr}\n",
      "\\toprule\n",
      " & algorithm & precision_summary & recall_summary & f1_summary & shd_summary & time \\\\\n",
      "\\midrule\n",
      "0 & micro-level & 0.875 & 1.000 & 0.933 & 2 & 92.645 \\\\\n",
      "1 & subgroups & 0.867 & 0.929 & 0.897 & 3 & 55.907 \\\\\n",
      "2 & group-embedding & 0.867 & 0.929 & 0.897 & 3 & 25.814 \\\\\n",
      "3 & pca+pcmci & 0.917 & 0.786 & 0.846 & 4 & 3.338 \\\\\n",
      "4 & pca+dynotears & 0.000 & 0.000 & 0.000 & 14 & 1.061 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = [pd.read_csv(f'results_{data_name}/{file}') for file in os.listdir(f'results_{data_name}') if file.endswith('.csv')]\n",
    "algorithms = [file.split('_')[1].split('.')[0] for file in os.listdir(f'results_{data_name}') if file.endswith('.csv')]\n",
    "\n",
    "for result, algorithm in zip(results, algorithms): result['algorithm'] = algorithm\n",
    "results = pd.concat(results, ignore_index=True)\n",
    "\n",
    "results = results[['algorithm', 'precision_summary', 'recall_summary', 'f1_summary', 'shd_summary', 'time']]\n",
    "# results\n",
    "print(results.to_latex(float_format=\"%.3f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal-inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
